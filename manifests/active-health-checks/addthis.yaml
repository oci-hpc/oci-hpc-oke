apiVersion: v1
kind: ConfigMap
metadata:
  name: active-health-checks-nccl-tests-bm-gpu4-8
  namespace: monitoring
data:
  job.yaml: |-
    apiVersion: kubeflow.org/v2beta1
    kind: MPIJob
    metadata:
      name: JOB_NAME_PLACEHOLDER
      namespace: monitoring
    spec:
      slotsPerWorker: 8
      launcherCreationPolicy: WaitForWorkersReady
      runPolicy:
        cleanPodPolicy: Running
      sshAuthMountPath: /root/.ssh
      mpiReplicaSpecs:
        Launcher:
          replicas: 1
          template:
            metadata:
              labels:
                active-health-checks-nccl-tests-job: JOB_NAME_PLACEHOLDER
                active-health-checks-nccl-tests-replica: mpi-launcher
            spec:
              priorityClassName: active-health-checks-low
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              restartPolicy: OnFailure
              terminationGracePeriodSeconds: 2
              # NODE_AFFINITY_PLACEHOLDER
              containers:
              - name: mpi-launcher
                image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-24.11-nccl-2.23.4-1
                command:
                - /bin/bash
                - -c
                - |
                  set -e -o pipefail; trap 'exit=1' SIGINT
                  NUM_GPUS=8
                  HOSTFILE=/etc/mpi/hostfile
                  EXPECTED=${WORKER_REPLICAS:-2}
                  until [ -s "$HOSTFILE" ] && [ "$(wc -l < "$HOSTFILE")" -ge "$EXPECTED" ]; do
                    echo "[launcher] Waiting for hostfile ($HOSTFILE) to list $EXPECTED workers..."
                    sleep 2
                  done
                  NUM_HOSTS=$(wc -l < "$HOSTFILE")
                  NP=$((NUM_HOSTS*NUM_GPUS))
                  mpirun --allow-run-as-root \
                    -mca coll ^hcoll -mca plm_rsh_args "-p 2222 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null" \
                    -mca coll_hcoll_enable 0 \
                    -np $NP -npernode $NUM_GPUS --bind-to numa \
                    -hostfile "$HOSTFILE" \
                    -x NCCL_DEBUG=WARN \
                    -x NCCL_IB_SPLIT_DATA_ON_QPS=0 \
                    -x NCCL_IB_QPS_PER_CONNECTION=4 \
                    -x NCCL_IB_GID_INDEX=3 \
                    -x NCCL_IB_HCA==mlx5_0,mlx5_2,mlx5_6,mlx5_8,mlx5_10,mlx5_12,mlx5_14,mlx5_16,mlx5_1,mlx5_3,mlx5_7,mlx5_9,mlx5_11,mlx5_13,mlx5_15,mlx5_17 \
                    -x NCCL_IB_TC=41 \
                    -x NCCL_IB_SL=0 \
                    -x NCCL_IB_TIMEOUT=22 \
                    -x HCOLL_ENABLE_MCAST_ALL=0 \
                    -x UCX_TLS=tcp \
                    -x UCX_NET_DEVICES=eth0 \
                    /workspace/nccl-tests/build/all_reduce_perf -b 1G -f 2 -g 1 -e 4G -c 1
                ports:
                - name: mpijob-port
                  containerPort: 2222
                  protocol: TCP
                resources:
                  limits:
                    ephemeral-storage: 32Gi
                  requests:
                    cpu: 8
                    ephemeral-storage: 32Gi
                    memory: 2Gi
                securityContext:
                  privileged: true
                  capabilities:
                    add:
                    - IPC_LOCK
                    - CAP_SYS_ADMIN
                volumeMounts:
                - mountPath: /dev/infiniband
                  name: devinf
                - mountPath: /dev/shm
                  name: shm
                workingDir: /workspace
              volumes:
              - name: devinf
                hostPath:
                  path: /dev/infiniband
              - name: shm
                emptyDir:
                  medium: Memory
                  sizeLimit: 32Gi
        Worker:
          replicas: 2
          template:
            metadata:
              labels:
                active-health-checks-nccl-tests-job: JOB_NAME_PLACEHOLDER
                active-health-checks-nccl-tests-replica: mpi-worker
            spec:
              priorityClassName: active-health-checks-low
              hostNetwork: true
              dnsPolicy: ClusterFirstWithHostNet
              restartPolicy: OnFailure
              terminationGracePeriodSeconds: 15
              tolerations:
              - key: nvidia.com/gpu
                operator: Exists
              # NODE_AFFINITY_PLACEHOLDER
              containers:
              - name: mpi-worker
                image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-24.11-nccl-2.23.4-1
                command:
                - /bin/bash
                - -c
                - mkdir -p /var/run/sshd; /usr/sbin/sshd -D -p 2222 || sleep 999999999;
                ports:
                - name: mpijob-port
                  containerPort: 2222
                  protocol: TCP
                resources:
                  limits:
                    nvidia.com/gpu: 8
                    ephemeral-storage: 32Gi
                  requests:
                    cpu: 64
                    ephemeral-storage: 32Gi
                    memory: 512Gi
                    nvidia.com/gpu: 8
                securityContext:
                  privileged: true
                  capabilities:
                    add:
                    - IPC_LOCK
                    - CAP_SYS_ADMIN
                volumeMounts:
                - mountPath: /dev/infiniband
                  name: devinf
                - mountPath: /dev/shm
                  name: shm
                workingDir: /workspace
              volumes:
              - name: devinf
                hostPath:
                  path: /dev/infiniband
              - name: shm
                emptyDir:
                  medium: Memory
                  sizeLimit: 32Gi