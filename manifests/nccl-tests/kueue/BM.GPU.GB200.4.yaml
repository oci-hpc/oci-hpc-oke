apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: bm-gpu-gb200-4
spec:
  nodeLabels:
    node.kubernetes.io/instance-type: BM.GPU.GB200.4
    nvidia.com/gpu: "true"
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: bm-gpu-gb200-4-nccl-tests-queue
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu", "ephemeral-storage"]
    flavors:
    - name: bm-gpu-gb200-4
      resources:
      - name: cpu
        nominalQuota: "20000"
      - name: memory
        nominalQuota: "102400Gi"
      - name: nvidia.com/gpu
        nominalQuota: "1600"
      - name: ephemeral-storage
        nominalQuota: "6400Gi"
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: bm-gpu-gb200-4-nccl-tests
spec:
  clusterQueue: bm-gpu-gb200-4-nccl-tests-queue
---
apiVersion: resource.nvidia.com/v1beta1
kind: ComputeDomain
metadata:
  name: bm-gpu-gb200-4-nccl-tests-compute-domain
spec:
  numNodes: 2
  channel:
    resourceClaimTemplate:
      name: bm-gpu-gb200-4-nccl-tests-compute-domain-channel
---
apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: nccl-test
  labels:
    kueue.x-k8s.io/queue-name: bm-gpu-gb200-4-nccl-tests
spec:
  slotsPerWorker: 4
  runPolicy:
    cleanPodPolicy: "Running"
  sshAuthMountPath: /root/.ssh
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        metadata:
          labels:
            nccl-test-replica: mpi-launcher
        spec:
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          containers:
          - name: mpi-launcher
            image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-25.03-nccl-2.26.6-1
            ports:
            - { name: mpijob-port, containerPort: 2222, protocol: TCP }
            command: ["bash", "-c"]
            args:
              - |
                NUM_GPUS=4
                NUM_HOSTS=$(sed -n '$=' /etc/mpi/hostfile)
                NP=$(($NUM_HOSTS*$NUM_GPUS))
                while ! (for host in $(awk '{print $1}' /etc/mpi/hostfile); do ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -p 2222 $host exit 2>/dev/null || exit 1; done); do
                  echo "Waiting for workers to be ready..."
                  sleep 5
                done
                echo "All workers are ready!"
                mpirun --allow-run-as-root -mca plm_rsh_args "-p 2222" \
                --bind-to none \
                --map-by ppr:4:node \
                --mca coll ^hcoll \
                -x NCCL_DEBUG=WARN \
                -x NCCL_MNNVL_ENABLE=1 \
                -x NCCL_CUMEM_ENABLE=1 \
                -x NCCL_NET_PLUGIN=sys \
                -x NCCL_IB_HCA=mlx5_0,mlx5_1,mlx5_3,mlx5_4 \
                -x NCCL_NVLS_ENABLE=1 \
                -x NCCL_SOCKET_IFNAME=eth0 \
                -np $NP \
                /workspace/nccl-tests/build/all_reduce_perf -b 8 -e 32G -f 2 -g 1
    Worker:
      replicas: 2
      template:
        metadata:
          labels:
            nccl-test-replica: mpi-worker
        spec:
          hostNetwork: true
          dnsPolicy: ClusterFirstWithHostNet
          volumes:
          - { name: devinf, hostPath: { path: /dev/infiniband }}
          - { name: shm, emptyDir: { medium: Memory, sizeLimit: 32Gi }}
          containers:
          - name: mpi-worker
            ports:
            - { name: mpijob-port, containerPort: 2222, protocol: TCP }
            volumeMounts:
            - { mountPath: /dev/infiniband, name: devinf }
            - { mountPath: /dev/shm, name: shm }
            securityContext:
              privileged: true
              capabilities:
                add: ["IPC_LOCK"]
            image: iad.ocir.io/hpc_limited_availability/nccl-tests:pytorch-25.03-nccl-2.26.6-1
            command:
              - /bin/bash
              - -c
              - mkdir -p /var/run/sshd; /usr/sbin/sshd -D -p 2222;
            resources:
              limits:
                nvidia.com/gpu: 4
              claims:
              - name: compute-domain-channel
          resourceClaims:
          - name: compute-domain-channel
            resourceClaimTemplateName: bm-gpu-gb200-4-nccl-tests-compute-domain-channel